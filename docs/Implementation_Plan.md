# 実装計画と改善提案 (Implementation Plan and Improvement Proposals)

本ドキュメントは、`simple_graphrag` リポジトリの現状の課題を特定し、その改善に向けた具体的な実装計画と設計提案をまとめたものです。GraphRAGシステムの精度、柔軟性、および拡張性を向上させることを目的とします。

## 改善点・今後の設計提案

### 1. エンティティ抽出精度の向上 (Enhancing Entity Extraction Accuracy)

*   **優先度**: 高
*   **期待される効果**: グラフのノイズが減少し、クエリの精度と再現性が向上する。特に専門用語や固有名詞の抽出精度が安定し、信頼性の高い知識グラフが構築される。

**現状の課題**: 現在はLLMに任せてエンティティを抽出していますが、抽出対象が限定されていないため、想定外のノードが生成される可能性があります。

**提案**:
*   **LLMGraphTransformer のパラメータ活用**: `LLMGraphTransformer` の `allowed_nodes` や `allowed_relationships` パラメータを指定することで、抽出対象を限定し、精度を高めます。例えば `allowed_nodes=["Person", "Organization", "Product", "Concept"]` のように制約することで、モデルが想定外のノードを作るのを防ぎます。
*   **エンティティノードへの概要と埋め込み付与**: GraphRAGの設計例にならって、各エンティティノードに概要（`description`）や埋め込み（`description_embedding`）を付与する手法を検討します。これらの要約や埋め込みは別途LLMで生成し、ノードプロパティとして格納することで、検索時の関連性向上や再利用性が期待できます。

**具体的な実装計画 (Concrete Implementation Plan)**:
1.  `src/data_ingestion.py` 内の `LLMGraphTransformer` の初期化部分を特定します。
2.  プロジェクトの要件に基づいて、抽出を許可するノードタイプと関係タイプを定義します。これらは設定ファイル（例: `src/config.py`）で管理することを検討します。
3.  `LLMGraphTransformer` のインスタンス化時に `allowed_nodes` および `allowed_relationships` パラメータを設定します。
4.  エンティティノードに `description` および `description_embedding` を追加するロジックを検討します。これは、グラフ変換後に別途LLMを呼び出して各ノードの概要を生成し、その概要を埋め込みモデルでベクトル化してノードプロパティとして追加する形になるでしょう。この処理は `data_ingestion.py` 内、または独立したモジュールとして実装します。
5.  既存のテスト（`tests/test_data_ingestion.py` など）を更新し、新しいノードタイプやプロパティが正しく抽出・格納されることを確認します。

### 2. Retrieval設計の強化 (Strengthening Retrieval Design)

*   **優先度**: 高
*   **期待される効果**: ユーザーの意図をより正確に捉えた動的なグラフ検索が可能になり、複雑な質問に対する回答精度が大幅に向上する。検索パフォーマンスも最適化される。

**現状の課題**: リトリーバーの改善には、グラフ検索とベクトル検索の重み付けや、より高度なクエリ生成・最適化が必要です。

**提案**:
*   **LangChainの GraphCypherQAChain の活用**: LLMがCypherクエリを生成してグラフ問合せを自動化し、回答生成チェーンを構築します。これにより、より複雑なグラフ構造に対する質問にも対応できるようになります。
*   **ハイブリッド検索の最適化**: 現在のハイブリッド検索（ベクトル＋グラフ）は基本方針として有効ですが、各取得結果のスコアリングやフィルタリング、パフォーマンス最適化（インデックス設計など）も検討すべきです。
*   **多段階検索の導入**: ユーザー質問中のキーワードや対話履歴を使って多段階検索を行う仕組みも、回答精度向上に貢献します。

**具体的な実装計画 (Concrete Implementation Plan)**:
1.  `src/query_handler.py` を中心に、現在のリトリーバーのロジックを分析します。
2.  `GraphCypherQAChain` の導入を検討し、LLMがCypherクエリを生成する部分を実装します。これには、Neo4jのスキーマ情報（ノードと関係のタイプ、プロパティ）をLLMに提供する必要があります。
3.  ハイブリッド検索の結果を統合するロジックを改善します。ベクトル検索とグラフ検索の結果をどのように組み合わせ、最終的な関連度スコアを算出するかを定義します。必要に応じて、再ランキングモジュールを導入します。
4.  Neo4jのインデックス設計を見直し、クエリパフォーマンスを最適化します。
5.  多段階検索のプロトタイプを実装します。これは、最初の検索結果に基づいて追加のクエリを生成し、より詳細な情報を取得するイテレーションプロセスを含む可能性があります。
6.  `tests/test_query_handler.py` を拡張し、新しいリトリーバーの挙動と精度を検証するテストケースを追加します。

### 3. ベクトル検索との融合 (Deeper Integration with Vector Search)

*   **優先度**: 中
*   **期待される効果**: グラフ構造の持つ関係性や文脈をベクトル空間で捉えることで、より意味的に深いレベルでの類似検索が可能になる。単なるテキストマッチングでは見つけられない関連性を発見できる可能性がある。

**現状の課題**: Neo4jのベクトルストアは利用していますが、ドキュメント以外のグラフ構造のベクトル化や、より高精度な埋め込みモデルの導入が可能です。

**提案**:
*   **グラフ埋め込み技術の検討**: ドキュメントだけでなくグラフ構造そのものをベクトル化する技術（例えばグラフ埋め込み）を組み合わせることで、グラフ全体から意味的に近いサブグラフを検索する手法の検討が可能です。
*   **高精度な埋め込みモデルの導入**: Ollamaや他のローカルモデルで利用できるより高精度な埋め込みモデルを導入することも、検索品質の向上につながります。
*   **ノード間関係を考慮した重み付け**: ノード間関係（隣接やパス長）を考慮した重み付けや、グラフサンプリング技術による検索支援も、ハイブリッドRAGの深化策として有効です。

**具体的な実装計画 (Concrete Implementation Plan)**:
1.  現在の埋め込みモデル（Ollama）の性能を評価し、より高精度なモデル（例: Sentence Transformers, OpenAI Embeddingsなど）への切り替えの可能性を調査します。これらは `src/config.py` で設定可能にするべきです。
2.  Neo4jVector の利用方法を再確認し、必要に応じて埋め込みの生成と格納プロセスを最適化します。
3.  グラフ埋め込み（Graph Embeddings）の概念を調査し、Neo4j Graph Data Science Library (GDS) などのツールを活用して、グラフ全体またはサブグラフの埋め込みを生成するPoC (Proof of Concept) を作成します。
4.  生成されたグラフ埋め込みを検索プロセスに統合する方法を検討します。例えば、ユーザーの質問とグラフ埋め込みの類似度を計算し、関連するサブグラフを特定するなどのアプローチが考えられます。
5.  ノード間の関係性（例: 距離、タイプ）に基づいて検索結果の重み付けを行うロジックを `src/query_handler.py` に追加します。
6.  関連するテストケースを `tests/test_connections.py` や `tests/test_query_handler.py` に追加し、新しいベクトル検索の挙動を検証します。

### 4. 各種LLMを利用できるようにする (Enabling Various LLMs)

*   **優先度**: 高
*   **期待される効果**: 特定のLLMベンダーにロックインされることなく、性能、コスト、ライセンス要件に応じて最適なモデルを柔軟に選択・切り替えられるようになる。システムのポータビリティと将来性が向上する。

**現状の課題**: 現在はOllamaに限定されていますが、LangChainの利点を活かし、より多様なLLMを利用できるようにする必要があります。

**提案**:
*   **LangChainのLLM抽象化レイヤーの活用**: LangChainが提供するLLMの抽象化レイヤーを利用し、Ollamaだけでなく、OpenAIやAzure OpenAIなどの各種LLMを容易に切り替えられるように実装します。

**具体的な実装計画 (Concrete Implementation Plan)**:
1.  `src/config.py` にLLMプロバイダー（例: `ollama`, `openai`, `azure_openai`）と、それぞれのAPIキーやエンドポイントなどの設定を追加します。
2.  `src/config.py` または新しいユーティリティモジュールに、設定に基づいて適切なLangChain LLMインスタンス（例: `ChatOllama`, `ChatOpenAI`, `AzureChatOpenAI`）を初期化するファクトリ関数を作成します。
3.  `app.py` および `src/query_handler.py` など、LLMが利用されているすべての箇所で、このファクトリ関数を使用してLLMインスタンスを取得するように変更します。これにより、LLMの切り替えが設定変更のみで可能になります。
4.  各LLMプロバイダーに対応する環境変数（例: `OPENAI_API_KEY`, `AZURE_OPENAI_API_KEY`）の読み込みと管理を `src/config.py` で行います。
5.  新しいLLMプロバイダーが正しく統合され、期待通りに動作することを確認するためのテストケースを `tests/test_connections.py` や `tests/test_app_backend.py` に追加します。

---

この計画に基づき、段階的にリポジトリの改善を進めていきます。各フェーズでテストと検証を行い、システムの安定性と性能を確保します。
